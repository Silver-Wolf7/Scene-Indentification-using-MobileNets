{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 08:29:33.675029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 08:29:39.918235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-20 08:29:39.918273: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-20 08:29:55.112013: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-20 08:29:55.113798: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-20 08:29:55.113821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, GlobalMaxPooling2D, Activation, Multiply, Conv2D, Concatenate, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed\n",
    "from keras.layers.core import Activation\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "\n",
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_labels(file):\n",
    "    labels_file = open(file, \"r\")\n",
    "    labels = []\n",
    "    \n",
    "    for line in labels_file:\n",
    "        label = line.strip()\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels_file.close()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "#list of all labels\n",
    "class_names = list_labels(\"./CamSDD/Labels.txt\")\n",
    "class_name_labels = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    Category = [\"training\", \"test\", \"validation\"]\n",
    "    output = []\n",
    "    \n",
    "    for category in Category:\n",
    "        print(\"Loading {}\".format(category))\n",
    "        path = os.path.join(folder, category)\n",
    "        print(path)\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for sub_folder in os.listdir(path):\n",
    "            label = class_name_labels[sub_folder]\n",
    "            \n",
    "            #Iterating through all images\n",
    "            for file in os.listdir(os.path.join(path, sub_folder)):\n",
    "                \n",
    "                #getting the image path\n",
    "                img_path = os.path.join(os.path.join(path, sub_folder), file)\n",
    "                \n",
    "                #appending image and corresponding label\n",
    "                images.append(cv2.resize(cv2.imread(img_path), (224, 224)))\n",
    "                # images.append(cv2.imread(img_path))\n",
    "                labels.append(label)\n",
    "            \n",
    "        #check that data type doesn't affect accuracy\n",
    "        images = (np.array(images, dtype='float32')/127.5) - 1\n",
    "        labels = np.array(labels, dtype='int8')\n",
    "        \n",
    "        output.append((images, labels))\n",
    "        \n",
    "    return output\n",
    "\n",
    "#channel module\n",
    "def channel_attention_module(x, ratio, bias):\n",
    "    b, _, _, channel = x.shape\n",
    "    # MLP shared layer\n",
    "    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=bias)\n",
    "    l2 = Dense(channel, use_bias=bias)\n",
    "    \n",
    "    # Global Average pooling\n",
    "    x1 = GlobalAveragePooling2D()(x)\n",
    "    x1 = l1(x1)\n",
    "    x1 = l2(x1)\n",
    "    \n",
    "    # Global Max pooling\n",
    "    x2 = GlobalMaxPooling2D()(x)\n",
    "    x2 = l1(x2)\n",
    "    x2 = l2(x2)\n",
    "    \n",
    "    # Adding both and applying sigmoid\n",
    "    features = x1 + x2\n",
    "    features = Activation(\"sigmoid\")(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# spatial attention module\n",
    "def spatial_attention_module(x):\n",
    "    # Average pooling\n",
    "    x1 = tf.reduce_mean(x, axis=-1)\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\n",
    "    \n",
    "    # Max pooling\n",
    "    x2 = tf.reduce_max(x, axis=-1)\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\n",
    "    \n",
    "    # Concatenate\n",
    "    features = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Conv layer\n",
    "    features = Conv2D(1, kernel_size=7, padding=\"same\", activation=\"sigmoid\")(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# CBAM\n",
    "def CBAM(x, ratio, bias):\n",
    "    x = channel_attention_module(x, ratio=ratio, bias=bias)\n",
    "    x = spatial_attention_module(x)\n",
    "    return x\n",
    "\n",
    "def elu2(x):\n",
    "    return K.switch(K.less_equal(x, 1), K.exp(x) - 1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training\n",
      "./CamSDD/training\n",
      "Loading test\n",
      "./CamSDD/test\n",
      "Loading validation\n",
      "./CamSDD/validation\n"
     ]
    }
   ],
   "source": [
    "#shuffling train data\n",
    "(train_images, train_labels), (test_images, test_labels), (validation_images, validation_labels)= load_data(\"./CamSDD\")\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "validation_images, validation_labels = shuffle(validation_images, validation_labels, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 06:11:51.709560: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-20 06:11:51.709636: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-20 06:11:51.709709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (feng-linux-08.leeds.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-02-20 06:11:51.711705: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 82s 157ms/step - loss: 2.3061 - accuracy: 0.4100 - val_loss: 0.9837 - val_accuracy: 0.8450\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 78s 158ms/step - loss: 0.9146 - accuracy: 0.7693 - val_loss: 0.5912 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 78s 157ms/step - loss: 0.6425 - accuracy: 0.8293 - val_loss: 0.4702 - val_accuracy: 0.8867\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.5034 - accuracy: 0.8612 - val_loss: 0.3992 - val_accuracy: 0.9017\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 77s 156ms/step - loss: 0.4266 - accuracy: 0.8819 - val_loss: 0.3583 - val_accuracy: 0.9067\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 77s 155ms/step - loss: 0.3581 - accuracy: 0.8984 - val_loss: 0.3290 - val_accuracy: 0.9117\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.3087 - accuracy: 0.9137 - val_loss: 0.3134 - val_accuracy: 0.9100\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2823 - accuracy: 0.9191 - val_loss: 0.2962 - val_accuracy: 0.9150\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.2415 - accuracy: 0.9314 - val_loss: 0.2755 - val_accuracy: 0.9250\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.2123 - accuracy: 0.9399 - val_loss: 0.2729 - val_accuracy: 0.9150\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1853 - accuracy: 0.9448 - val_loss: 0.2625 - val_accuracy: 0.9183\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1630 - accuracy: 0.9530 - val_loss: 0.2616 - val_accuracy: 0.9183\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1437 - accuracy: 0.9613 - val_loss: 0.2514 - val_accuracy: 0.9183\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1229 - accuracy: 0.9675 - val_loss: 0.2431 - val_accuracy: 0.9183\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1034 - accuracy: 0.9727 - val_loss: 0.2439 - val_accuracy: 0.9250\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0923 - accuracy: 0.9752 - val_loss: 0.2413 - val_accuracy: 0.9300\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0774 - accuracy: 0.9823 - val_loss: 0.2438 - val_accuracy: 0.9283\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0646 - accuracy: 0.9872 - val_loss: 0.2441 - val_accuracy: 0.9267\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0540 - accuracy: 0.9879 - val_loss: 0.2391 - val_accuracy: 0.9300\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0469 - accuracy: 0.9893 - val_loss: 0.2440 - val_accuracy: 0.9233\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0410 - accuracy: 0.9927 - val_loss: 0.2438 - val_accuracy: 0.9283\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0347 - accuracy: 0.9938 - val_loss: 0.2335 - val_accuracy: 0.9300\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0282 - accuracy: 0.9956 - val_loss: 0.2538 - val_accuracy: 0.9200\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0240 - accuracy: 0.9966 - val_loss: 0.2454 - val_accuracy: 0.9233\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0218 - accuracy: 0.9973 - val_loss: 0.2539 - val_accuracy: 0.9217\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0212 - accuracy: 0.9961 - val_loss: 0.2534 - val_accuracy: 0.9233\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0163 - accuracy: 0.9985 - val_loss: 0.2478 - val_accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNet(weights='imagenet', include_top=False) \n",
    "\n",
    "x=base_model.output\n",
    "x=CBAM(x, 4, True)\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024, activation='sigmoid')(x)\n",
    "x=Dropout(0.7)(x)\n",
    "output = Dense(30, activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r4_T/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r4_T/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 216ms/step - loss: 0.2395 - accuracy: 0.9233\n",
      "Test loss: 0.23949870467185974\n",
      "Test accuracy: 0.9233333468437195\n"
     ]
    }
   ],
   "source": [
    "model.save(\"CBAM_r4_T\")\n",
    "score = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 79s 155ms/step - loss: 2.3059 - accuracy: 0.4096 - val_loss: 0.9835 - val_accuracy: 0.8417\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 77s 156ms/step - loss: 0.9148 - accuracy: 0.7682 - val_loss: 0.5918 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 77s 155ms/step - loss: 0.6428 - accuracy: 0.8299 - val_loss: 0.4708 - val_accuracy: 0.8883\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 77s 155ms/step - loss: 0.5034 - accuracy: 0.8617 - val_loss: 0.3998 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 77s 155ms/step - loss: 0.4263 - accuracy: 0.8813 - val_loss: 0.3587 - val_accuracy: 0.9083\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 77s 155ms/step - loss: 0.3576 - accuracy: 0.8984 - val_loss: 0.3298 - val_accuracy: 0.9100\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.3082 - accuracy: 0.9142 - val_loss: 0.3141 - val_accuracy: 0.9067\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2821 - accuracy: 0.9188 - val_loss: 0.2959 - val_accuracy: 0.9117\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2413 - accuracy: 0.9310 - val_loss: 0.2752 - val_accuracy: 0.9233\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.2123 - accuracy: 0.9404 - val_loss: 0.2732 - val_accuracy: 0.9133\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1854 - accuracy: 0.9451 - val_loss: 0.2630 - val_accuracy: 0.9183\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1633 - accuracy: 0.9530 - val_loss: 0.2631 - val_accuracy: 0.9150\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.1440 - accuracy: 0.9609 - val_loss: 0.2526 - val_accuracy: 0.9167\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1228 - accuracy: 0.9672 - val_loss: 0.2457 - val_accuracy: 0.9200\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1038 - accuracy: 0.9727 - val_loss: 0.2479 - val_accuracy: 0.9250\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0932 - accuracy: 0.9759 - val_loss: 0.2458 - val_accuracy: 0.9267\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0784 - accuracy: 0.9815 - val_loss: 0.2488 - val_accuracy: 0.9250\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.0646 - accuracy: 0.9864 - val_loss: 0.2495 - val_accuracy: 0.9183\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 76s 155ms/step - loss: 0.0547 - accuracy: 0.9877 - val_loss: 0.2418 - val_accuracy: 0.9317\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0468 - accuracy: 0.9902 - val_loss: 0.2508 - val_accuracy: 0.9200\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0413 - accuracy: 0.9929 - val_loss: 0.2463 - val_accuracy: 0.9300\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0348 - accuracy: 0.9937 - val_loss: 0.2376 - val_accuracy: 0.9250\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0284 - accuracy: 0.9959 - val_loss: 0.2550 - val_accuracy: 0.9250\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0247 - accuracy: 0.9964 - val_loss: 0.2529 - val_accuracy: 0.9283\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0219 - accuracy: 0.9970 - val_loss: 0.2510 - val_accuracy: 0.9250\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0212 - accuracy: 0.9963 - val_loss: 0.2530 - val_accuracy: 0.9267\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.2527 - val_accuracy: 0.9183\n"
     ]
    }
   ],
   "source": [
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "base_model2=MobileNet(weights='imagenet', include_top=False) \n",
    "x2=base_model2.output\n",
    "x2=CBAM(x2, 4, False)\n",
    "x2=GlobalAveragePooling2D()(x2)\n",
    "x2=Dense(1024, activation='sigmoid')(x2)\n",
    "x2=Dropout(0.7)(x2)\n",
    "output2 = Dense(30, activation=\"softmax\")(x2)\n",
    "\n",
    "model2=Model(inputs=base_model2.input,outputs=output2)\n",
    "\n",
    "for layer in base_model2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model2.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history2 = model2.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor2], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r4_F/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r4_F/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 210ms/step - loss: 0.2429 - accuracy: 0.9250\n",
      "Test loss: 0.24288415908813477\n",
      "Test accuracy: 0.925000011920929\n"
     ]
    }
   ],
   "source": [
    "model2.save(\"CBAM_r4_F\")\n",
    "score2 = model2.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 78s 154ms/step - loss: 2.3285 - accuracy: 0.4040 - val_loss: 0.9717 - val_accuracy: 0.8367\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.9097 - accuracy: 0.7691 - val_loss: 0.5888 - val_accuracy: 0.8783\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.6519 - accuracy: 0.8244 - val_loss: 0.4740 - val_accuracy: 0.8917\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.5226 - accuracy: 0.8542 - val_loss: 0.4056 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.4512 - accuracy: 0.8726 - val_loss: 0.3668 - val_accuracy: 0.9067\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.3912 - accuracy: 0.8899 - val_loss: 0.3399 - val_accuracy: 0.9083\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.3471 - accuracy: 0.9008 - val_loss: 0.3237 - val_accuracy: 0.9117\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.3266 - accuracy: 0.9074 - val_loss: 0.3062 - val_accuracy: 0.9150\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2931 - accuracy: 0.9128 - val_loss: 0.2882 - val_accuracy: 0.9183\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2718 - accuracy: 0.9221 - val_loss: 0.2834 - val_accuracy: 0.9183\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2497 - accuracy: 0.9255 - val_loss: 0.2736 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2323 - accuracy: 0.9310 - val_loss: 0.2705 - val_accuracy: 0.9200\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2190 - accuracy: 0.9332 - val_loss: 0.2614 - val_accuracy: 0.9217\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2012 - accuracy: 0.9390 - val_loss: 0.2514 - val_accuracy: 0.9233\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1838 - accuracy: 0.9469 - val_loss: 0.2492 - val_accuracy: 0.9283\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1746 - accuracy: 0.9485 - val_loss: 0.2471 - val_accuracy: 0.9283\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1569 - accuracy: 0.9540 - val_loss: 0.2438 - val_accuracy: 0.9300\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1463 - accuracy: 0.9580 - val_loss: 0.2394 - val_accuracy: 0.9283\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1325 - accuracy: 0.9622 - val_loss: 0.2312 - val_accuracy: 0.9350\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1231 - accuracy: 0.9659 - val_loss: 0.2258 - val_accuracy: 0.9350\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.1125 - accuracy: 0.9679 - val_loss: 0.2307 - val_accuracy: 0.9367\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.1070 - accuracy: 0.9695 - val_loss: 0.2275 - val_accuracy: 0.9333\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.0924 - accuracy: 0.9759 - val_loss: 0.2355 - val_accuracy: 0.9333\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.0846 - accuracy: 0.9784 - val_loss: 0.2345 - val_accuracy: 0.9367\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.0816 - accuracy: 0.9785 - val_loss: 0.2297 - val_accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "base_model3=MobileNet(weights='imagenet', include_top=False) \n",
    "x3=base_model3.output\n",
    "x3=CBAM(x3, 16, True)\n",
    "x3=GlobalAveragePooling2D()(x3)\n",
    "x3=Dense(1024, activation='sigmoid')(x3)\n",
    "x3=Dropout(0.7)(x3)\n",
    "output3 = Dense(30, activation=\"softmax\")(x3)\n",
    "\n",
    "model3=Model(inputs=base_model3.input,outputs=output3)\n",
    "\n",
    "for layer in base_model3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor3 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model3.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history3 = model3.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor3], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r16_T/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r16_T/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 219ms/step - loss: 0.2361 - accuracy: 0.9317\n",
      "Test loss: 0.23611727356910706\n",
      "Test accuracy: 0.9316666722297668\n"
     ]
    }
   ],
   "source": [
    "model3.save(\"CBAM_r16_T\")\n",
    "score3 = model3.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score3[0])\n",
    "print('Test accuracy:', score3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 08:32:20.802728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-20 08:32:20.802767: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-20 08:32:20.802807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (feng-linux-03.leeds.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-02-20 08:32:20.804034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 57s 110ms/step - loss: 2.3287 - accuracy: 0.4042 - val_loss: 0.9710 - val_accuracy: 0.8367\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 54s 109ms/step - loss: 0.9091 - accuracy: 0.7690 - val_loss: 0.5883 - val_accuracy: 0.8783\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.6518 - accuracy: 0.8241 - val_loss: 0.4740 - val_accuracy: 0.8917\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.5224 - accuracy: 0.8538 - val_loss: 0.4057 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.4511 - accuracy: 0.8731 - val_loss: 0.3669 - val_accuracy: 0.9083\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.3911 - accuracy: 0.8907 - val_loss: 0.3398 - val_accuracy: 0.9067\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.3470 - accuracy: 0.9010 - val_loss: 0.3235 - val_accuracy: 0.9117\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.3264 - accuracy: 0.9076 - val_loss: 0.3062 - val_accuracy: 0.9167\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.2930 - accuracy: 0.9134 - val_loss: 0.2879 - val_accuracy: 0.9183\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.2719 - accuracy: 0.9218 - val_loss: 0.2827 - val_accuracy: 0.9183\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.2497 - accuracy: 0.9255 - val_loss: 0.2731 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.2325 - accuracy: 0.9309 - val_loss: 0.2706 - val_accuracy: 0.9217\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.2191 - accuracy: 0.9337 - val_loss: 0.2614 - val_accuracy: 0.9250\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.2013 - accuracy: 0.9382 - val_loss: 0.2514 - val_accuracy: 0.9233\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.1840 - accuracy: 0.9471 - val_loss: 0.2492 - val_accuracy: 0.9283\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.1748 - accuracy: 0.9478 - val_loss: 0.2461 - val_accuracy: 0.9283\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1572 - accuracy: 0.9536 - val_loss: 0.2444 - val_accuracy: 0.9283\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1466 - accuracy: 0.9577 - val_loss: 0.2400 - val_accuracy: 0.9283\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1328 - accuracy: 0.9617 - val_loss: 0.2316 - val_accuracy: 0.9350\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1234 - accuracy: 0.9654 - val_loss: 0.2266 - val_accuracy: 0.9367\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1127 - accuracy: 0.9682 - val_loss: 0.2311 - val_accuracy: 0.9383\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.1073 - accuracy: 0.9695 - val_loss: 0.2284 - val_accuracy: 0.9333\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.0926 - accuracy: 0.9765 - val_loss: 0.2369 - val_accuracy: 0.9333\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.0849 - accuracy: 0.9780 - val_loss: 0.2358 - val_accuracy: 0.9367\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 52s 105ms/step - loss: 0.0817 - accuracy: 0.9786 - val_loss: 0.2304 - val_accuracy: 0.9350\n"
     ]
    }
   ],
   "source": [
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "base_model4=MobileNet(weights='imagenet', include_top=False) \n",
    "x4=base_model4.output\n",
    "x4=CBAM(x4, 16, False)\n",
    "x4=GlobalAveragePooling2D()(x4)\n",
    "x4=Dense(1024, activation='sigmoid')(x4)\n",
    "x4=Dropout(0.7)(x4)\n",
    "output4 = Dense(30, activation=\"softmax\")(x4)\n",
    "\n",
    "model4=Model(inputs=base_model4.input,outputs=output4)\n",
    "\n",
    "for layer in base_model4.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor4 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model4.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history4 = model4.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor4], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r16_F/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r16_F/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 148ms/step - loss: 0.2356 - accuracy: 0.9300\n",
      "Test loss: 0.23561060428619385\n",
      "Test accuracy: 0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "model4.save(\"CBAM_r16_F\")\n",
    "score4 = model4.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score4[0])\n",
    "print('Test accuracy:', score4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 56s 109ms/step - loss: 2.3389 - accuracy: 0.4010 - val_loss: 0.9932 - val_accuracy: 0.8400\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.9212 - accuracy: 0.7676 - val_loss: 0.5952 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.6533 - accuracy: 0.8259 - val_loss: 0.4759 - val_accuracy: 0.8883\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.5185 - accuracy: 0.8562 - val_loss: 0.4058 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.4449 - accuracy: 0.8734 - val_loss: 0.3637 - val_accuracy: 0.9067\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.3820 - accuracy: 0.8940 - val_loss: 0.3354 - val_accuracy: 0.9117\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.3370 - accuracy: 0.9038 - val_loss: 0.3191 - val_accuracy: 0.9167\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.3137 - accuracy: 0.9087 - val_loss: 0.3037 - val_accuracy: 0.9133\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.2767 - accuracy: 0.9210 - val_loss: 0.2862 - val_accuracy: 0.9183\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 54s 108ms/step - loss: 0.2538 - accuracy: 0.9267 - val_loss: 0.2792 - val_accuracy: 0.9150\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.2289 - accuracy: 0.9328 - val_loss: 0.2704 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.2092 - accuracy: 0.9401 - val_loss: 0.2703 - val_accuracy: 0.9133\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1940 - accuracy: 0.9425 - val_loss: 0.2613 - val_accuracy: 0.9183\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1734 - accuracy: 0.9504 - val_loss: 0.2541 - val_accuracy: 0.9167\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1556 - accuracy: 0.9549 - val_loss: 0.2497 - val_accuracy: 0.9200\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.1443 - accuracy: 0.9588 - val_loss: 0.2544 - val_accuracy: 0.9200\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1263 - accuracy: 0.9667 - val_loss: 0.2538 - val_accuracy: 0.9167\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1145 - accuracy: 0.9679 - val_loss: 0.2504 - val_accuracy: 0.9183\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1000 - accuracy: 0.9724 - val_loss: 0.2457 - val_accuracy: 0.9250\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.0909 - accuracy: 0.9772 - val_loss: 0.2445 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0803 - accuracy: 0.9803 - val_loss: 0.2462 - val_accuracy: 0.9267\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0750 - accuracy: 0.9808 - val_loss: 0.2388 - val_accuracy: 0.9200\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 54s 109ms/step - loss: 0.0619 - accuracy: 0.9855 - val_loss: 0.2507 - val_accuracy: 0.9200\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 54s 108ms/step - loss: 0.0548 - accuracy: 0.9885 - val_loss: 0.2581 - val_accuracy: 0.9267\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 55s 111ms/step - loss: 0.0502 - accuracy: 0.9896 - val_loss: 0.2445 - val_accuracy: 0.9200\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0445 - accuracy: 0.9911 - val_loss: 0.2393 - val_accuracy: 0.9233\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 54s 108ms/step - loss: 0.0411 - accuracy: 0.9916 - val_loss: 0.2465 - val_accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "base_model5=MobileNet(weights='imagenet', include_top=False) \n",
    "x5=base_model5.output\n",
    "x5=CBAM(x5, 8, True)\n",
    "x5=GlobalAveragePooling2D()(x5)\n",
    "x5=Dense(1024, activation='sigmoid')(x5)\n",
    "x5=Dropout(0.7)(x5)\n",
    "output5 = Dense(30, activation=\"softmax\")(x5)\n",
    "\n",
    "model5=Model(inputs=base_model5.input,outputs=output5)\n",
    "\n",
    "for layer in base_model5.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor5 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model5.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history5 = model5.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor5], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r8_T/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r8_T/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 150ms/step - loss: 0.2365 - accuracy: 0.9233\n",
      "Test loss: 0.23650220036506653\n",
      "Test accuracy: 0.9233333468437195\n"
     ]
    }
   ],
   "source": [
    "model5.save(\"CBAM_r8_T\")\n",
    "score5 = model5.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score5[0])\n",
    "print('Test accuracy:', score5[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 56s 110ms/step - loss: 2.3386 - accuracy: 0.4004 - val_loss: 0.9928 - val_accuracy: 0.8400\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.9213 - accuracy: 0.7678 - val_loss: 0.5952 - val_accuracy: 0.8733\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.6526 - accuracy: 0.8260 - val_loss: 0.4753 - val_accuracy: 0.8900\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.5182 - accuracy: 0.8558 - val_loss: 0.4058 - val_accuracy: 0.8983\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.4447 - accuracy: 0.8734 - val_loss: 0.3633 - val_accuracy: 0.9050\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.3823 - accuracy: 0.8936 - val_loss: 0.3350 - val_accuracy: 0.9133\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.3369 - accuracy: 0.9040 - val_loss: 0.3181 - val_accuracy: 0.9133\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.3135 - accuracy: 0.9081 - val_loss: 0.3036 - val_accuracy: 0.9133\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.2767 - accuracy: 0.9202 - val_loss: 0.2854 - val_accuracy: 0.9200\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.2540 - accuracy: 0.9276 - val_loss: 0.2795 - val_accuracy: 0.9183\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 52s 106ms/step - loss: 0.2293 - accuracy: 0.9323 - val_loss: 0.2699 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.2095 - accuracy: 0.9391 - val_loss: 0.2696 - val_accuracy: 0.9133\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.1943 - accuracy: 0.9416 - val_loss: 0.2607 - val_accuracy: 0.9200\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.1739 - accuracy: 0.9500 - val_loss: 0.2533 - val_accuracy: 0.9167\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1560 - accuracy: 0.9553 - val_loss: 0.2493 - val_accuracy: 0.9183\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.1446 - accuracy: 0.9585 - val_loss: 0.2525 - val_accuracy: 0.9200\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.1265 - accuracy: 0.9666 - val_loss: 0.2527 - val_accuracy: 0.9200\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 54s 109ms/step - loss: 0.1147 - accuracy: 0.9684 - val_loss: 0.2486 - val_accuracy: 0.9233\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 54s 109ms/step - loss: 0.1003 - accuracy: 0.9738 - val_loss: 0.2450 - val_accuracy: 0.9233\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 54s 109ms/step - loss: 0.0910 - accuracy: 0.9765 - val_loss: 0.2426 - val_accuracy: 0.9217\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0804 - accuracy: 0.9800 - val_loss: 0.2457 - val_accuracy: 0.9233\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0751 - accuracy: 0.9807 - val_loss: 0.2384 - val_accuracy: 0.9217\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0621 - accuracy: 0.9857 - val_loss: 0.2503 - val_accuracy: 0.9167\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0551 - accuracy: 0.9887 - val_loss: 0.2568 - val_accuracy: 0.9200\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 54s 108ms/step - loss: 0.0503 - accuracy: 0.9897 - val_loss: 0.2415 - val_accuracy: 0.9200\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0445 - accuracy: 0.9909 - val_loss: 0.2368 - val_accuracy: 0.9217\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0411 - accuracy: 0.9912 - val_loss: 0.2413 - val_accuracy: 0.9267\n",
      "Epoch 28/100\n",
      "495/495 [==============================] - 53s 108ms/step - loss: 0.0341 - accuracy: 0.9939 - val_loss: 0.2397 - val_accuracy: 0.9250\n",
      "Epoch 29/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0311 - accuracy: 0.9950 - val_loss: 0.2485 - val_accuracy: 0.9217\n",
      "Epoch 30/100\n",
      "495/495 [==============================] - 53s 107ms/step - loss: 0.0250 - accuracy: 0.9964 - val_loss: 0.2391 - val_accuracy: 0.9217\n",
      "Epoch 31/100\n",
      "495/495 [==============================] - 53s 106ms/step - loss: 0.0244 - accuracy: 0.9959 - val_loss: 0.2416 - val_accuracy: 0.9283\n"
     ]
    }
   ],
   "source": [
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "base_model6=MobileNet(weights='imagenet', include_top=False) \n",
    "x6=base_model6.output\n",
    "x6=CBAM(x6, 8, False)\n",
    "x6=GlobalAveragePooling2D()(x6)\n",
    "x6=Dense(1024, activation='sigmoid')(x6)\n",
    "x6=Dropout(0.7)(x6)\n",
    "output6 = Dense(30, activation=\"softmax\")(x6)\n",
    "\n",
    "model6=Model(inputs=base_model6.input,outputs=output6)\n",
    "\n",
    "for layer in base_model6.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "monitor6 = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model6.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history6 = model6.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor6], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r8_F/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CBAM_r8_F/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 147ms/step - loss: 0.2472 - accuracy: 0.9183\n",
      "Test loss: 0.24724383652210236\n",
      "Test accuracy: 0.9183333516120911\n"
     ]
    }
   ],
   "source": [
    "model6.save(\"CBAM_r8_F\")\n",
    "score6 = model6.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score6[0])\n",
    "print('Test accuracy:', score6[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60f60e76100a664673c60b5022fd886353d7910fb19790531071192a56dbe781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
