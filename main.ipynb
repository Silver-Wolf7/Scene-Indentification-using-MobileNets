{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:07:18.090428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 13:07:18.248747: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 13:07:18.253077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-04-15 13:07:18.253094: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-15 13:07:20.659917: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-04-15 13:07:20.660031: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-04-15 13:07:20.660042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, GlobalMaxPooling2D, Activation, Multiply, Conv2D, Concatenate, Flatten, ELU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed\n",
    "import re\n",
    "from keras import backend as K\n",
    "\n",
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_labels(file):\n",
    "    labels_file = open(file, \"r\")\n",
    "    labels = []\n",
    "    \n",
    "    for line in labels_file:\n",
    "        label = line.strip()\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels_file.close()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "#list of all labels\n",
    "class_names = list_labels(\"./CamSDD/Labels.txt\")\n",
    "class_name_labels = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "\n",
    "\n",
    "def load_data(folder):\n",
    "    Category = [\"training\", \"test\", \"validation\"]\n",
    "    output = []\n",
    "    \n",
    "    for category in Category:\n",
    "        print(\"Loading {}\".format(category))\n",
    "        path = os.path.join(folder, category)\n",
    "        print(path)\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for sub_folder in os.listdir(path):\n",
    "            label = class_name_labels[sub_folder]\n",
    "            \n",
    "            #Iterating through all images\n",
    "            for file in os.listdir(os.path.join(path, sub_folder)):\n",
    "                \n",
    "                #getting the image path\n",
    "                img_path = os.path.join(os.path.join(path, sub_folder), file)\n",
    "                \n",
    "                #appending image and corresponding label\n",
    "                images.append(cv2.resize(cv2.imread(img_path), (224, 224)))\n",
    "                # images.append(cv2.imread(img_path))\n",
    "                labels.append(label)\n",
    "            \n",
    "        #check that data type doesn't affect accuracy\n",
    "        images = (np.array(images, dtype='float32')/127.5)-1\n",
    "        labels = np.array(labels, dtype='int8')\n",
    "        \n",
    "        output.append((images, labels))\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "#displays 25 images with labels\n",
    "#cite\n",
    "def display_examples(class_names, images, labels):\n",
    "    figsize = (20, 20)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(\"Example of images\", fontsize=16)\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i].astype(np.uint8))\n",
    "        plt.xlabel(class_names[labels[i]])\n",
    "    plt.show()\n",
    "\n",
    "#https://stackoverflow.com/questions/49492255/how-to-replace-or-insert-intermediate-layer-in-keras-model\n",
    "def insert_layer_nonseq(model, layer_regex, insert_layer_factory,\n",
    "                        insert_layer_name=None, position='after'):\n",
    "\n",
    "    # Auxiliary dictionary to describe the network graph\n",
    "    network_dict = {'input_layers_of': {}, 'new_output_tensor_of': {}}\n",
    "\n",
    "    # Set the input layers of each layer\n",
    "    for layer in model.layers:\n",
    "        for node in layer._outbound_nodes:\n",
    "            layer_name = node.outbound_layer.name\n",
    "            if layer_name not in network_dict['input_layers_of']:\n",
    "                network_dict['input_layers_of'].update(\n",
    "                        {layer_name: [layer.name]})\n",
    "            else:\n",
    "                network_dict['input_layers_of'][layer_name].append(layer.name)\n",
    "\n",
    "    # Set the output tensor of the input layer\n",
    "    network_dict['new_output_tensor_of'].update(\n",
    "            {model.layers[0].name: model.input})\n",
    "\n",
    "    # Iterate over all layers after the input\n",
    "    model_outputs = []\n",
    "    for layer in model.layers[1:]:\n",
    "\n",
    "        # Determine input tensors\n",
    "        layer_input = [network_dict['new_output_tensor_of'][layer_aux] \n",
    "                for layer_aux in network_dict['input_layers_of'][layer.name]]\n",
    "        if len(layer_input) == 1:\n",
    "            layer_input = layer_input[0]\n",
    "\n",
    "        # Insert layer if name matches the regular expression\n",
    "        if re.match(layer_regex, layer.name):\n",
    "            if position == 'replace':\n",
    "                x = layer_input\n",
    "            elif position == 'after':\n",
    "                x = layer(layer_input)\n",
    "            elif position == 'before':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError('position must be: before, after or replace')\n",
    "\n",
    "            new_layer = insert_layer_factory()\n",
    "            if insert_layer_name:\n",
    "                new_layer._name = insert_layer_name\n",
    "            else:\n",
    "                new_layer._name = '{}_{}'.format(layer.name, \n",
    "                                                new_layer.name)\n",
    "            x = new_layer(x)\n",
    "            print('New layer: {} Old layer: {} Type: {}'.format(new_layer.name,\n",
    "                                                            layer.name, position))\n",
    "            if position == 'before':\n",
    "                x = layer(x)\n",
    "        else:\n",
    "            x = layer(layer_input)\n",
    "\n",
    "        # Set new output tensor (the original one, or the one of the inserted\n",
    "        # layer)\n",
    "        network_dict['new_output_tensor_of'].update({layer.name: x})\n",
    "\n",
    "        # Save tensor in output list if it is output in initial model\n",
    "        if layer_name in model.output_names:\n",
    "            model_outputs.append(x)\n",
    "\n",
    "    return Model(inputs=model.inputs, outputs=model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training\n",
      "./CamSDD/training\n",
      "Loading test\n",
      "./CamSDD/test\n",
      "Loading validation\n",
      "./CamSDD/validation\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels), (validation_images, validation_labels)= load_data(\"./CamSDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling train data\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "validation_images, validation_labels = shuffle(validation_images, validation_labels, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 220ms/step - loss: 0.1666 - accuracy: 0.9567 - sparse_top_k_categorical_accuracy: 0.9933\n",
      "Test loss: 0.16657833755016327\n",
      "Test accuracy: 0.9566666483879089\n",
      "time:  0:00:05.115132\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('spatial_configB5')\n",
    "\n",
    "start = datetime.now()\n",
    "score = model.evaluate(test_images, test_labels)\n",
    "end = datetime.now()\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('time: ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(test_images[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "model.predict_classes(test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_examples(class_names, train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel module\n",
    "#channel module\n",
    "def channel_attention_module(x, ratio, bias):\n",
    "    b, _, _, channel = x.shape\n",
    "    # MLP shared layer\n",
    "    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=bias)\n",
    "    l2 = Dense(channel, use_bias=bias)\n",
    "    \n",
    "    # Global Average pooling\n",
    "    x1 = GlobalAveragePooling2D()(x)\n",
    "    x1 = l1(x1)\n",
    "    x1 = l2(x1)\n",
    "    \n",
    "    # Global Max pooling\n",
    "    x2 = GlobalMaxPooling2D()(x)\n",
    "    x2 = l1(x2)\n",
    "    x2 = l2(x2)\n",
    "    \n",
    "    # Adding both and applying sigmoid\n",
    "    features = x1 + x2\n",
    "    features = Activation(\"sigmoid\")(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# spatial attention module\n",
    "def spatial_attention_module(x, kernel_size=7, bias=True):\n",
    "    # Average pooling\n",
    "    x1 = tf.reduce_mean(x, axis=-1)\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\n",
    "    \n",
    "    # Max pooling\n",
    "    x2 = tf.reduce_max(x, axis=-1)\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\n",
    "    \n",
    "    # Concatenate\n",
    "    features = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Conv layer\n",
    "    features = Conv2D(1, kernel_size=kernel_size, use_bias=bias, padding=\"same\", activation=\"sigmoid\")(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# CBAM\n",
    "def CBAM(x, ratio, bias):\n",
    "    x = channel_attention_module(x, ratio=ratio, bias=bias)\n",
    "    x = spatial_attention_module(x)\n",
    "    return x\n",
    "\n",
    "def elu2(x):\n",
    "    return K.switch(K.less_equal(x, 1), K.exp(x) - 1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 16:43:42.207006: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cserv1_a/soc_ug/fy19iars/project/lib/python3.9/site-packages/cv2/../../lib64:/uollinapps/AppsData/src/vscode/1.71.2-1663191299.el7/lib64\n",
      "2023-02-25 16:43:42.207042: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-25 16:43:42.207064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (feng-linux-08.leeds.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2023-02-25 16:43:42.207480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNet(weights='imagenet', include_top=False) \n",
    "\n",
    "# def elu_layer():\n",
    "#     return ELU(name='elu')\n",
    "# base_model = insert_layer_nonseq(base_model, '.*relu.*', elu_layer, position=\"replace\")\n",
    "\n",
    "x=base_model.output\n",
    "x=spatial_attention_module(x, 9, True)\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024, activation='sigmoid')(x)\n",
    "x=Dropout(0.7)(x)\n",
    "output = Dense(30, activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking model architecture\n",
    "print(model.summary())\n",
    "# for i,layer in enumerate(model.layers):\n",
    "#   print(i,layer.name)\n",
    "# model.layers[3].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "495/495 [==============================] - 78s 154ms/step - loss: 2.2866 - accuracy: 0.4194 - val_loss: 1.0155 - val_accuracy: 0.8417\n",
      "Epoch 2/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.9527 - accuracy: 0.7650 - val_loss: 0.6102 - val_accuracy: 0.8733\n",
      "Epoch 3/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.6652 - accuracy: 0.8252 - val_loss: 0.4790 - val_accuracy: 0.8850\n",
      "Epoch 4/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.5404 - accuracy: 0.8544 - val_loss: 0.4107 - val_accuracy: 0.8967\n",
      "Epoch 5/100\n",
      "495/495 [==============================] - 74s 151ms/step - loss: 0.4617 - accuracy: 0.8670 - val_loss: 0.3715 - val_accuracy: 0.9017\n",
      "Epoch 6/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.4070 - accuracy: 0.8856 - val_loss: 0.3441 - val_accuracy: 0.9050\n",
      "Epoch 7/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.3666 - accuracy: 0.8970 - val_loss: 0.3224 - val_accuracy: 0.9117\n",
      "Epoch 8/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.3300 - accuracy: 0.9027 - val_loss: 0.3115 - val_accuracy: 0.9083\n",
      "Epoch 9/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.3055 - accuracy: 0.9103 - val_loss: 0.2938 - val_accuracy: 0.9167\n",
      "Epoch 10/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.2833 - accuracy: 0.9156 - val_loss: 0.2874 - val_accuracy: 0.9167\n",
      "Epoch 11/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2688 - accuracy: 0.9216 - val_loss: 0.2796 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.2523 - accuracy: 0.9272 - val_loss: 0.2696 - val_accuracy: 0.9217\n",
      "Epoch 13/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.2353 - accuracy: 0.9329 - val_loss: 0.2651 - val_accuracy: 0.9250\n",
      "Epoch 14/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.2212 - accuracy: 0.9350 - val_loss: 0.2632 - val_accuracy: 0.9183\n",
      "Epoch 15/100\n",
      "495/495 [==============================] - 76s 154ms/step - loss: 0.2097 - accuracy: 0.9379 - val_loss: 0.2600 - val_accuracy: 0.9267\n",
      "Epoch 16/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1976 - accuracy: 0.9410 - val_loss: 0.2559 - val_accuracy: 0.9267\n",
      "Epoch 17/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1914 - accuracy: 0.9414 - val_loss: 0.2480 - val_accuracy: 0.9267\n",
      "Epoch 18/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1730 - accuracy: 0.9514 - val_loss: 0.2456 - val_accuracy: 0.9300\n",
      "Epoch 19/100\n",
      "495/495 [==============================] - 76s 153ms/step - loss: 0.1720 - accuracy: 0.9476 - val_loss: 0.2412 - val_accuracy: 0.9317\n",
      "Epoch 20/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1651 - accuracy: 0.9512 - val_loss: 0.2377 - val_accuracy: 0.9317\n",
      "Epoch 21/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1561 - accuracy: 0.9530 - val_loss: 0.2424 - val_accuracy: 0.9267\n",
      "Epoch 22/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1483 - accuracy: 0.9548 - val_loss: 0.2417 - val_accuracy: 0.9283\n",
      "Epoch 23/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1380 - accuracy: 0.9597 - val_loss: 0.2397 - val_accuracy: 0.9300\n",
      "Epoch 24/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1352 - accuracy: 0.9609 - val_loss: 0.2348 - val_accuracy: 0.9300\n",
      "Epoch 25/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1350 - accuracy: 0.9600 - val_loss: 0.2327 - val_accuracy: 0.9333\n",
      "Epoch 26/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.1268 - accuracy: 0.9629 - val_loss: 0.2347 - val_accuracy: 0.9300\n",
      "Epoch 27/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.1181 - accuracy: 0.9646 - val_loss: 0.2315 - val_accuracy: 0.9333\n",
      "Epoch 28/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.1088 - accuracy: 0.9689 - val_loss: 0.2311 - val_accuracy: 0.9283\n",
      "Epoch 29/100\n",
      "495/495 [==============================] - 75s 151ms/step - loss: 0.1106 - accuracy: 0.9680 - val_loss: 0.2315 - val_accuracy: 0.9300\n",
      "Epoch 30/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.1029 - accuracy: 0.9668 - val_loss: 0.2321 - val_accuracy: 0.9317\n",
      "Epoch 31/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.0971 - accuracy: 0.9702 - val_loss: 0.2344 - val_accuracy: 0.9300\n",
      "Epoch 32/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.0969 - accuracy: 0.9738 - val_loss: 0.2285 - val_accuracy: 0.9333\n",
      "Epoch 33/100\n",
      "495/495 [==============================] - 75s 152ms/step - loss: 0.0892 - accuracy: 0.9744 - val_loss: 0.2287 - val_accuracy: 0.9300\n",
      "Epoch 34/100\n",
      "495/495 [==============================] - 74s 150ms/step - loss: 0.0865 - accuracy: 0.9760 - val_loss: 0.2325 - val_accuracy: 0.9267\n",
      "Epoch 35/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0825 - accuracy: 0.9770 - val_loss: 0.2330 - val_accuracy: 0.9300\n",
      "Epoch 36/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0835 - accuracy: 0.9770 - val_loss: 0.2315 - val_accuracy: 0.9267\n",
      "Epoch 37/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0751 - accuracy: 0.9793 - val_loss: 0.2353 - val_accuracy: 0.9300\n",
      "Epoch 38/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0738 - accuracy: 0.9800 - val_loss: 0.2290 - val_accuracy: 0.9267\n",
      "Epoch 39/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0718 - accuracy: 0.9809 - val_loss: 0.2313 - val_accuracy: 0.9333\n",
      "Epoch 40/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0714 - accuracy: 0.9806 - val_loss: 0.2255 - val_accuracy: 0.9350\n",
      "Epoch 41/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0657 - accuracy: 0.9821 - val_loss: 0.2298 - val_accuracy: 0.9283\n",
      "Epoch 42/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0664 - accuracy: 0.9827 - val_loss: 0.2284 - val_accuracy: 0.9300\n",
      "Epoch 43/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0613 - accuracy: 0.9836 - val_loss: 0.2289 - val_accuracy: 0.9317\n",
      "Epoch 44/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0609 - accuracy: 0.9826 - val_loss: 0.2304 - val_accuracy: 0.9333\n",
      "Epoch 45/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0589 - accuracy: 0.9844 - val_loss: 0.2250 - val_accuracy: 0.9333\n",
      "Epoch 46/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0529 - accuracy: 0.9869 - val_loss: 0.2400 - val_accuracy: 0.9300\n",
      "Epoch 47/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0541 - accuracy: 0.9846 - val_loss: 0.2333 - val_accuracy: 0.9367\n",
      "Epoch 48/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0539 - accuracy: 0.9866 - val_loss: 0.2340 - val_accuracy: 0.9350\n",
      "Epoch 49/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0524 - accuracy: 0.9863 - val_loss: 0.2348 - val_accuracy: 0.9383\n",
      "Epoch 50/100\n",
      "495/495 [==============================] - 74s 149ms/step - loss: 0.0493 - accuracy: 0.9870 - val_loss: 0.2383 - val_accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 212ms/step - loss: 0.2206 - accuracy: 0.9233\n",
      "Test loss: 0.22056683897972107\n",
      "Test accuracy: 0.9233333468437195\n"
     ]
    }
   ],
   "source": [
    "# # # model.save(\"spatial_k9_T_2\")\n",
    "# model1 = keras.models.load_model('spatial_k7_T')\n",
    "\n",
    "score = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    # #plot accuracy\n",
    "    # plt.subplot(221)\n",
    "    # plt.plot(history.history[\"accuracy\"], 'bo--', label=\"acc\")\n",
    "    # plt.plot(history.history[\"val_accuracy\"], \"ro--\", label = \"val_acc\")\n",
    "    # plt.title(\"train_acc vs val_acc\")\n",
    "    # plt.ylabel(\"accuracy\")\n",
    "    # plt.xlabel(\"epochs\")\n",
    "    # plt.legend()\n",
    "    \n",
    "    #plot loss function\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history[\"loss\"], 'bo--', label=\"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], \"ro--\", label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60f60e76100a664673c60b5022fd886353d7910fb19790531071192a56dbe781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
