{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.applications import MobileNet\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, GlobalMaxPooling2D, Activation, Multiply, Conv2D, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import SparseTopKCategoricalAccuracy\n",
    "from numpy.random import seed\n",
    "from helper_functions import list_labels, load_data, insert_layer_nonseq, inference_time\n",
    "import seaborn as sn\n",
    "# import mobilenet\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three attention mechanisms cited from https://github.com/nikhilroxtomar/Attention-Mechanism-Implementation.git\n",
    "# channel module\n",
    "def channel_attention_module(x, ratio, bias):\n",
    "    b, _, _, channel = x.shape\n",
    "    # MLP shared layer\n",
    "    l1 = Dense(channel//ratio, activation=\"relu\", use_bias=bias)\n",
    "    l2 = Dense(channel, use_bias=bias)\n",
    "    \n",
    "    # Global Average pooling\n",
    "    x1 = GlobalAveragePooling2D()(x)\n",
    "    x1 = l1(x1)\n",
    "    x1 = l2(x1)\n",
    "    \n",
    "    # Global Max pooling\n",
    "    x2 = GlobalMaxPooling2D()(x)\n",
    "    x2 = l1(x2)\n",
    "    x2 = l2(x2)\n",
    "    \n",
    "    # Adding both and applying sigmoid\n",
    "    features = x1 + x2\n",
    "    features = Activation(\"sigmoid\")(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# spatial attention module\n",
    "def spatial_attention_module(x, kernel_size=7, bias=True):\n",
    "    # Average pooling\n",
    "    x1 = tf.reduce_mean(x, axis=-1)\n",
    "    x1 = tf.expand_dims(x1, axis=-1)\n",
    "    \n",
    "    # Max pooling\n",
    "    x2 = tf.reduce_max(x, axis=-1)\n",
    "    x2 = tf.expand_dims(x2, axis=-1)\n",
    "    \n",
    "    # Concatenate\n",
    "    features = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Conv layer\n",
    "    features = Conv2D(1, kernel_size=kernel_size, padding=\"same\", activation=\"sigmoid\", use_bias=bias)(features)\n",
    "    features = Multiply()([x, features])\n",
    "    \n",
    "    return features\n",
    "    \n",
    "# CBAM\n",
    "def CBAM(x, ratio=16, bias=True):\n",
    "    x = channel_attention_module(x, ratio=ratio, bias=bias)\n",
    "    x = spatial_attention_module(x)\n",
    "    return x\n",
    "\n",
    "def CBAM_factory():\n",
    "    return CBAM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and shuffling dataset\n",
    "#list of all labels\n",
    "class_names = list_labels(\"./CamSDD/Labels.txt\")\n",
    "class_name_labels = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels), (validation_images, validation_labels)= load_data(\"./CamSDD\", class_name_labels)\n",
    "train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "validation_images, validation_labels = shuffle(validation_images, validation_labels, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed\n",
    "seed(25)\n",
    "tf.random.set_seed(25)\n",
    "tf.keras.utils.set_random_seed(25)\n",
    "\n",
    "#creating model using mobilenet function\n",
    "base_model = MobileNet(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "'''\n",
    "uncomment below to use mobilenet with custom activations\n",
    "'''\n",
    "# base_model = mobilenet.MobileNet(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "'''\n",
    "Uncomment the line below to insert attention modules\n",
    "The line below only adds a single attention\n",
    "The second parameter has to be modified to add more \n",
    "'''\n",
    "# base_model = insert_layer_nonseq(base_model, '.*pw_(13)_relu.*', CBAM_factory)\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024, activation='sigmoid')(x)\n",
    "x=Dropout(0.7)(x)\n",
    "output = Dense(30, activation=\"softmax\")(x)\n",
    "model=Model(inputs=base_model.input,outputs=output)\n",
    "\n",
    "'''\n",
    "Comment the two line below to not freeze the model\n",
    "'''\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# training the model with an early stopping monitor\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=[\"accuracy\", SparseTopKCategoricalAccuracy(k=3)])\n",
    "history = model.fit(train_images, train_labels, batch_size=20, epochs=100, validation_data=(validation_images,validation_labels), callbacks=[monitor], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uncomment the line below to load a saved model\n",
    "'''\n",
    "model = keras.models.load_model('CBAM_combined_relu')\n",
    "\n",
    "score = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "average_inference_time = inference_time(model, test_images, test_labels)\n",
    "print('Inference time:', average_inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "test_pred = model.predict(test_images)\n",
    "test_pred = np.argmax (test_pred, axis = 1)\n",
    "result = confusion_matrix(test_labels, test_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using seaborn to visualise confusion matrix\n",
    "plt.figure(figsize = (10,6))\n",
    "ids = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "results = pd.DataFrame(result, index = [i for i in ids],\n",
    "                  columns = [i for i in ids])\n",
    "ax = sn.heatmap(results, annot=True, cmap=sn.cubehelix_palette(as_cmap=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "60f60e76100a664673c60b5022fd886353d7910fb19790531071192a56dbe781"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
